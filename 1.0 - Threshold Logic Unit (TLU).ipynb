{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCTION\n",
    "\n",
    "This is a series of implementations of relevant models in Machine Learning. The implementation pipeline starts with a brief theoretical exposition of the model followed by a detailed implementation and associated discussion/analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Threshold Logic Unit (TLU) and Perceptron Learning Rule\n",
    "\n",
    "**1.1 Brief on Threshold Logic Unit:** The Threshold Logic Unit (TLU) is a basic form of machine learning model consisting of a single *input unit* (and corresponding weights), and an *activation function*. Note that the TLU is the most basic form of AI-neuron/computational unit, knowedge of which will lay the foundation for advanced topics in machine learning and deep learning. \n",
    "Each input $x_i$, is associated with a *weight* $w_i$, in which the sum of the weighted inputs (products of the input-weight $x_i\\times w_i$) is computed to decide the activation, $a$:\n",
    "                                    $$a = \\sum_{i=1}^n x_iw_i$$\n",
    "\n",
    "While the inputs remain unchange, the weights are randomly initialised and are adjusted through a training technique. For the TLU, the training process relies on a pair of examples -- $(x_i,y_i)$ corresponding to an arbitrary datapoint $(x_i)$ and its class $(y_i)$. This form of learning is referred to as *supervised learning* because both the data instance and the target are used in the learning process. Other forms of learning are *unsupervised* (utilises the input to infer relevant clusters or categories) and *reinforcement* (motivates and rewards the model for a correct prediction, hence the model is aimed at maximising reward), which I will not belabour here. The model is said to learn if it can correctly classify a previously unseen datapoint.\n",
    "\n",
    "A threshold is defined $\\theta$ (often user-defined) and the model produces an output if the threshold is exceeded, otherwise no output.\n",
    "The final output or prediction is based on the sum of the weighted inputs. The TLU is based on mimicking the functionality of biological neuron at high-level. A typical neuron receives multitude of inputs from afferent neurons, each associated with weight. The weighted-inputs are modulated in the receiving neuron (efferent) and the neuron respond accordingly -- fires/produces a pulse (1) or no firing/no pulse (0).\n",
    "This is achieved in the TLU via an activation function which takes the $a$ as input to generate a prediction $\\hat{y}$, as follows: \n",
    "    $$\\hat{y} = 1 \\mbox{ if } a \\geq \\theta  \\mbox{ otherwise } \\hat{y} = 0$$\n",
    "\n",
    "**Weights adjustments in the TLU:** From the perspective of a learning model datapoints belong to groups and there exist one or more decision surface(s) that demarcate the groups or classes. Thus, the goal of a learning model is to achieve a certain task such as classification of objects after the training regime. During training, the model identifies a set of parameters or free parameters (e.g. weights) to be used in conjunction with the input to achieve the desired goal by identifying the decision surface. It is crucial in any ML-based model to identify the free parameters, i.e. parameters to learn in helping the model to identify the decision surface.\n",
    "The threshold, which was initially a scalar quantity is used as a baseline or *bias* (i.e., the baseline to attain before the neuron fires), is treated as a weight with a constant input of -1 ($x_iw_i >\\theta$; after transformation $x_iw_i + (-1)\\theta = 0 $) and integrated in the main stream of input-weight for model training. A learning rule is defined and repeatedly executed until the right setting of the parameter is obtained (at this stage the model is expected to correctly classify new examples). For each training epoch/regime, a marginal change is made to the weights. Adjustment to the weights vector is a function of the output of the given instance. On that basis, the parameters are adjusted - either increased or decreased according to the learning rule. \n",
    "\n",
    "**Some technical descriptions**\n",
    "\n",
    "With the augmented threshold, the action of the TLU is either positve or negative given by: \n",
    "    $$ \\mathbf{w.x} \\geq 0 \\longrightarrow y=1  \\mbox{ or } \\mathbf{w.x} < 0 \\longrightarrow y=0 $$  \n",
    "\n",
    "Because the input vector $\\mathbf{x}$ is not affected during the training process, i.e. remains unchanged, only the weight vector $\\mathbf{w}$ is adjusted to align properly with the input vector (see illustration). Using a learning rate $\\alpha \\mbox{ s.t. } 0<\\alpha<1$, to control the process, a new vector $\\mathbf{w'}$ is formed which is closer to the input vector $\\mathbf{x}$. According to the decision rule, adjusting the weight can be based on addition or subtracting the weight vector; since both are likely, a learning rule that combines both is used instead. Thus,\n",
    "    $$\\mathbf{w'} =\\mathbf{w} + \\alpha\\mathbf{x} \\mbox{ or } \\mathbf{w'}= \\mathbf{w}-\\alpha\\mathbf{x}$$\n",
    "A combine form is given by:\n",
    "    $$\\mathbf{w'} = \\mathbf{w}+\\alpha(\\mathbf{t-y})\\mathbf{x}$$\n",
    "Where $t-y$ is used to decide the adjustment direction. The relationsip above can be expressed in a number of ways: (1) In terms of change in weight vector: \n",
    "    $$\\delta \\mathbf{w} =\\mathbf{w'}-\\mathbf{w}$  \\mbox{ but } $\\mathbf{w'} = \\mathbf{w} + \\alpha(\\mathbf{t-y})\\mathbf{v})$$\n",
    "    $$\\delta\\mathbf{w} = \\alpha(\\mathbf{t-y})\\mathbf{v})$$ \n",
    "(2) or its components: \n",
    "    $$\\delta\\mathbf{w_i} = \\alpha(\\mathbf{t_i-y_i})\\mathbf{v_i}) \\mbox{ where } $i = 1 to n+1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I.2 TLU Implementation**\n",
    "\n",
    "Having established the theoretical base and some running examples, the next step is to describe and implement the training phase of the model. **Perceptron Learning Rule:** The implementation here is based on the *perceptron training rule*, which is an advanced version of the TLU.\n",
    "$\\begin{verbatim}\n",
    "repeat\n",
    "    for each training vector pair (x,t)\n",
    "        evaluate the output y when \\mathbf{v} is input to the TLU\n",
    "        if y \\neq t then\n",
    "            form a new weight vector \\mathbf{w'} according to equation *\n",
    "        else\n",
    "            do nothing\n",
    "        end if\n",
    "    end for\n",
    " until  y = t \\mbox{ for all vectors }\n",
    "\\end{verbatim}$\n",
    "\n",
    "*Perceptron Convergence Theorem:* The use of perceptron learning rule is guaranteed to generate a valid weight vector that separates linearly separable pair of vector X, Y. More formally, the theorem states: \n",
    "\n",
    "*\"If two classes of vector X,Y are linearly separable, then application of the perceptron training algorithm will eventually result in a weight vector $\\mathbf{w_0}$ such that $\\mathbf{w_0}$ define a TLU whose decision hyperplane separate X and Y\"* -- Kevin Gurney, pp. 43.\n",
    "\n",
    "Basically, the implementation is based on the following steps:\n",
    "- identify inputs (and correct representation 0/1 or real values)\n",
    "- identify the free parameters/learnable parameters\n",
    "- specify/prescribe the learning rule -- optimisation\n",
    "- adjust parameters\n",
    "- test onvergence\n",
    "- evaluate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT RELEVANT PACKAGES:\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as m\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLU(object):\n",
    "    \"\"\"implementation of TLU\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size):\n",
    "        \"\"\"this function initialises the parameters needed to operationalise the class object\"\"\"\n",
    "        \n",
    "        self.weights = np.zeros(input_size+1) # weights and the corresponding threshold/bias\n",
    "        #self.epochs = epochs\n",
    "        #self.lrate = lrate\n",
    "        # if explict threshold value is given:\n",
    "        #self.weights = np.insert(self.weights, 0, threshold) # if explicit threshold is to be used\n",
    "        # FOR VISUALISATIONS:\n",
    "        self.pred_error = [] # prediction error (y-axis)\n",
    "        self.n_iter = [] #n_iter # number of iteration (x-axis)\n",
    "        \n",
    "    \n",
    "    def activate_tlu(self, x):\n",
    "        \"\"\"an activation function that returns 1 or 0\"\"\"\n",
    "        return 1 if x>= 0 else 0    \n",
    "    \n",
    "    def predict_tlu(self, row):\n",
    "        \"\"\"this function predicts individual row in a given dataset\"\"\"\n",
    "        \n",
    "        xw = np.array(row).dot(self.weights) # a  dot product of the row and weigth vectors\n",
    "    \n",
    "        a = self.activate_tlu(xw) # compute the activation using the 'activation' function ... \n",
    "        return a\n",
    "            \n",
    "        \n",
    "    def train_tlu(self, data, targets, epochs, lrate):\n",
    "        \"\"\"training the model to identify the right setting for the weight vector\"\"\"\n",
    "        \n",
    "        for epoch in range(epochs): # training to adjust the weights:\n",
    "            #print('Iteration: ', epoch)\n",
    "            for row, t in zip(data, targets):\n",
    "                row = np.insert(row, 0,-1) # insert the constant input (-1) for the bias\n",
    "                \n",
    "                pred = self.predict_tlu(row) # invoke the prediction function\n",
    "                error = t - pred # prediction error!\n",
    "                #update result for visualisation:\n",
    "                self.pred_error.append(error)\n",
    "                self.n_iter.append(epoch)\n",
    "                ##SEE SOME OUTPUTS ... to check the actual results:#print('Actual: ',t, ' Predicted: ',pred)\n",
    "                \n",
    "                # adjust the weights vector ...\n",
    "                if pred != t:\n",
    "                    for r in range(len(self.weights)): # or: row.shape[0]\n",
    "                        self.weights[r] = self.weights[r] + (lrate*error*row[r])\n",
    "                else:\n",
    "                    continue # do nothing and continue!\n",
    "\n",
    "        return self.weights # learned weights for future prediction ... \n",
    "        \n",
    "    def __str__(self): # print messages ... \n",
    "        #print('Actual: ',self.t, ' Predicted: ',self.pred)\n",
    "        return('TLU Iteration!\\n')\n",
    "    \n",
    "    # view result:\n",
    "    def view_result(self):\n",
    "        plt.scatter(self.n_iter, self.pred_error)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # a function to call instantiate training and prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to call instantiate training and prediction:\n",
    "def tlu_prediction(model, data, targets, epochs, lrate=0.2, toPrint=True):\n",
    "    # initialise the model to invoke relevant functions ...\n",
    "    adj_w = model.train_tlu(data, targets, epochs, lrate) \n",
    "    \n",
    "    if toPrint: # some visuals ... \n",
    "        print(model)# prints whatever is in the __str__()\n",
    "        print('Main Targets: ',targets,'\\n')\n",
    "        print('Main Inputs: ',data,'\\n')\n",
    "        print('Adjusted weights: ', adj_w, '\\n')\n",
    "        \n",
    "    return data, targets, adj_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #DATASET: Identify input and free parameters .... \n",
    "    \n",
    "Irrespective of the task/problem to solve, the input needs to be transformed to real values or binary values. We consider basic input vectors given by $\\mathbf{x_1} = [0 0 1 1]$ and $\\mathbf{x_2} = [0 1 0 1]$ . The free parameter(s) is the weight vector and is randomly initialised to kickstart the learning process ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGICAL AND DATA:\n",
    "andData = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "andTargets = np.array([0,0,0,1])\n",
    "# LGICAL OR DATA:\n",
    "orData = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "orTargets = np.array([0,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # invoke the function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TLU(input_size= 2)# class instantiation ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TLU Iteration!\n",
      "\n",
      "Main Targets:  [0 0 0 1] \n",
      "\n",
      "Main Inputs:  [[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]] \n",
      "\n",
      "Adjusted weights:  [0.9 0.6 0.3] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 1]]), array([0, 0, 0, 1]), array([0.9, 0.6, 0.3]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#USING AND DATA:\n",
    "#for i in range(5):\n",
    "tlu_prediction(model, andData, andTargets, epochs=11,lrate=0.3)\n",
    "#model.view_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TLU Iteration!\n",
      "\n",
      "Main Targets:  [0 1 1 1] \n",
      "\n",
      "Main Inputs:  [[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]] \n",
      "\n",
      "Adjusted weights:  [0.6 0.6 0.6] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 1]]), array([0, 1, 1, 1]), array([0.6, 0.6, 0.6]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#USING OR DATA:\n",
    "#for i in range(5):\n",
    "tlu_prediction(model, orData, orTargets, epochs=11,lrate=0.3)\n",
    "#model.view_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#sample visualisation:\n",
    "model.view_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #More Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file and name the columns:\n",
    "df = pd.read_csv('data/sonar.all-data', names=['V'+str(i) for i in range(61)])\n",
    "#rename the last column and binarize the entries:\n",
    "df = df.rename(columns = {'V60':'Class'}) # rename the last column\n",
    "df['Class'] = df.Class.apply(lambda x: 1 if x=='R' else 0)\n",
    "#save file:\n",
    "df.to_csv('data/sonar.all-data.csv', index_label=False)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full training is achieved using a stochastic gradient method .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets have a contrived data:\n",
    "data = []\n",
    "while len(data)<6:\n",
    "    for j in range(9): # to index to column and slice entries ..\n",
    "        l =[]\n",
    "        for i in range(3): # to index columns ...\n",
    "            l.extend([df.iloc[j,i]])\n",
    "        data.append(l)\n",
    "# append a random class to each data instance ... \n",
    "for r in data:\n",
    "    r.extend([np.random.choice(2)]) # random values -- 0/1\n",
    "# initialise a vector of random weights ... \n",
    "weights = np.random.rand(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the targets from the data:\n",
    "[i.remove(i[-1]) for i in data]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "########stop here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALTERNATE TRAINING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # ...these assume we have perfect/correct/right ettings for the weights to correctly classify a given linearly separable data. How do we get/obtai the set of the right weight vectors? This achived via an iterative process that utilises the input and initial set of parameter/free-metters -- such process is termed training/learning process which follows a prescribed instruction/set of rules ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Delta rule is used to adjust the weight of a single-layer neural netork based on the gradient \n",
    "# descent algorithm ... it is more effective to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE DELTA RULE\n",
    "The *perceptron learning rule*, which was used to train the single-layer TLU, is not suitable for handling weights in multilayer network. The weights in TLU are somewhat directed to follow the solution trajectory.\n",
    "\n",
    "Another important learning rule is commonly used to adjust weight vectors is *delta rule*. The *delta rule* is a function of the difference between the actual output from the network $\\hat{y}$ and the corresponding target $t$ given by $e_i = (t_i - \\hat{y_i})$. Each prediction is associated with an error and such error are integrated (considered to have equal weight) to get the total error, \n",
    "    $$E = \\frac{1}{N} \\sum_{i=1}^N e^i$$\n",
    "This difference is modelled as an error function to minimise through the weights adjustment. In other words, the error sum is a function of the weights in the network and the delta rule ensures that the minimum of the sum of errors over the training set. Essentially, the error function depends on the number of free parameters in the problem.\n",
    "\n",
    "*Gradient descent on an error function:* To descent down the error function or minimise the prediction error, the only free parameter in this instance is the weight; thus, various process are involve. Adjusting the weight to achieve minimum error is achieved using the *gradient descent* method: \n",
    "    $$\\delta w_i = - \\alpha\\frac{\\partial E}{\\partial w_i}$$ \n",
    "\n",
    "To apply gradient descent it is assumed that the function to minimise depends on the free parameters in a smooth, continous fashion. But we often take a detour in the minimisation procedure, i.e. the error is computed somewhat not in the manner required to directly apply gradient descent by examining various instance where the function could be *smooth* and *continous*. Lets examine the following senarious that rely on the weight vector:\n",
    "\n",
    "- is the error $E$ dependence on the $\\mathbf{w}$ smooth and continous? \n",
    "- is the activation $a$ dependence on the $\\mathbf{w}$ smooth and continuos? \n",
    "- is the prediction $\\hat{y}$ dependence on the $\\mathbf{w}$ smooth and continuous?\n",
    "\n",
    "In the first scenario, the activation $a = x\\times w$ directly relies on the weight vector since the input remains unchanged, thus it is smooth and continuous. However, the final prediction $\\hat{y}$, which is modulated using the step function, relies on the intermediary step function (creating a chain of dependencies). The *step function* is discontinous (not differentiable), which makes gradient descent infeasible to use in minimising the error $E$.\n",
    "To overcome the obstacle, the intermediate *step function* is dropped such that $e_i =(t_i-a_i)^2$ i.e. using the activation directly instead of the output ($\\hat{y}$), which relies on the discontinous step function. For simplicity $e_i = \\frac{1}{2}(t_i-a_i)^2)$ is represented by: \n",
    "    $$E = \\frac{1}{N}\\sum_{i=1}^Ne_i$$ \n",
    "    $$E = \\frac{1}{2N}\\sum_{i=1}^N(t_i-a_i)^2$$ \n",
    "\n",
    "The optimisation of the error function ($E$) requires seeing all the training instances. Sometimes the batch training, which is computational expansion for obvious reason is used. For a more efficient implementation, the weights are individually adjusted at the given instance of the error function given by: \n",
    "    $$\\frac{\\partial e_i}{\\partial w_i} = - (t_i-a_i)x_i$$ \n",
    "In order to change or adjust the weight vector, the following learning rule is used: \n",
    "    $$\\delta w_i = -\\alpha(t_i-a_i)x_i$$\n",
    "\n",
    "Some advantages of the *delta rule* over the *perceptron rule* include:\n",
    "- while the perceptron tends to continuosly oscillates in search of solution (hence the convergence theorem), especially when dealing with non-linearly separable data, the *delta rule* always converges to a solution (if the data is linearly separable) and does not oscillates continuously;\n",
    "- the *delta rule* has wider scope (based on $\\pm$), which enables it to effect little changes according to the learning rate $\\alpha$ and converges to a solution. On the other hand, the percpetron does not make further adjustment to the weights once a solution is found.\n",
    "- the delta rule is borne out of gradient descent and the percpetron borne out of vector manipulation and line equation\n",
    "- the delta rule is modelled to be diffenrentiable while the percepron learning rule is based on a discontinous step function which is not differentiable\n",
    "- both are useful learning rules to solve for classification problems involving linearlly seprable data. \n",
    "\n",
    "**Mitigating the discontinuity resulting from the use of a step function** \n",
    "\n",
    "The problem of discontinuity as a result if using the discontinuous step function is mitigated by using a smooth and differential squashing function. This reinstitate the output $y$ using the activation function known as the *sigmoid*, which is differentiable. . With this development, the derivative of the sigmoid is introduced in the main relation: $\\delta w_i = \\alpha\\sigma'(a)(t^p-y^p)x_i^p$ where $\\sigma'(a)$ or $\\frac{d\\sigma(a)}{da}$ is the derivative of the sigmoid with respect to the activation $a$ .... change to the weight (and the error) are communicated via the activation ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE DELTA RULE\n",
    "The *perceptron learning rule*, which was used to train the single-layer TLU, is not suitable for handling weights in multilayer network. The weights in TLU are somewhat directed to follow the solution trajectory.\n",
    "\n",
    "Another important learning rule is commonly used to adjust weight vectors is *delta rule*. The *delta rule* is a function of the difference between the actual output from the network $\\hat{y}$ and the corresponding target $t$ given by $e_i = (t_i - \\hat{y_i})$. Each prediction is associated with an error and such error are integrated (considered to have equal weight) to get the total error, \n",
    "    $$E = \\frac{1}{N} \\sum_{i=1}^N e^i$$\n",
    "This difference is modelled as an error function to minimise through the weights adjustment. In other words, the error sum is a function of the weights in the network and the delta rule ensures that the minimum of the sum of errors over the training set. Essentially, the error function depends on the number of free parameters in the problem.\n",
    "\n",
    "*Gradient descent on an error function:* To descent down the error function or minimise the prediction error, the only free parameter in this instance is the weight; thus, various process are involve. Adjusting the weight to achieve minimum error is achieved using the *gradient descent* method: \n",
    "    $$\\delta w_i = - \\alpha\\frac{\\partial E}{\\partial w_i}$$ \n",
    "\n",
    "To apply gradient descent it is assumed that the function to minimise depends on the free parameters in a smooth, continous fashion. But we often take a detour in the minimisation procedure, i.e. the error is computed somewhat not in the manner required to directly apply gradient descent by examining various instance where the function could be *smooth* and *continous*. Lets examine the following senarious that rely on the weight vector:\n",
    "\n",
    "- is the error $E$ dependence on the $\\mathbf{w}$ smooth and continous? \n",
    "- is the activation $a$ dependence on the $\\mathbf{w}$ smooth and continuos? \n",
    "- is the prediction $\\hat{y}$ dependence on the $\\mathbf{w}$ smooth and continuous?\n",
    "\n",
    "In the first scenario, the activation $a = x\\times w$ directly relies on the weight vector since the input remains unchanged, thus it is smooth and continuous. However, the final prediction $\\hat{y}$, which is modulated using the step function, relies on the intermediary step function (creating a chain of dependencies). The *step function* is discontinous (not differentiable), which makes gradient descent infeasible to use in minimising the error $E$.\n",
    "To overcome the obstacle, the intermediate *step function* is dropped such that $e_i =(t_i-a_i)^2$ i.e. using the activation directly instead of the output ($\\hat{y}$), which relies on the discontinous step function. For simplicity $e_i = \\frac{1}{2}(t_i-a_i)^2)$ is represented by: \n",
    "    $$E = \\frac{1}{N}\\sum_{i=1}^Ne_i$$ \n",
    "    $$E = \\frac{1}{2N}\\sum_{i=1}^N(t_i-a_i)^2$$ \n",
    "\n",
    "The optimisation of the error function ($E$) requires seeing all the training instances. Sometimes the batch training, which is computational expansion for obvious reason is used. For a more efficient implementation, the weights are individually adjusted at the given instance of the error function given by: \n",
    "    $$\\frac{\\partial e_i}{\\partial w_i} = - (t_i-a_i)x_i$$ \n",
    "In order to change or adjust the weight vector, the following learning rule is used: \n",
    "    $$\\delta w_i = -\\alpha(t_i-a_i)x_i$$\n",
    "\n",
    "Some advantages of the *delta rule* over the *perceptron rule* include:\n",
    "- while the perceptron tends to continuosly oscillates in search of solution (hence the convergence theorem), especially when dealing with non-linearly separable data, the *delta rule* always converges to a solution (if the data is linearly separable) and does not oscillates continuously;\n",
    "- the *delta rule* has wider scope (based on $\\pm$), which enables it to effect little changes according to the learning rate $\\alpha$ and converges to a solution. On the other hand, the percpetron does not make further adjustment to the weights once a solution is found.\n",
    "- the delta rule is borne out of gradient descent and the percpetron borne out of vector manipulation and line equation\n",
    "- the delta rule is modelled to be diffenrentiable while the percepron learning rule is based on a discontinous step function which is not differentiable\n",
    "- both are useful learning rules to solve for classification problems involving linearlly seprable data. \n",
    "\n",
    "**Mitigating the discontinuity resulting from the use of a step function** \n",
    "\n",
    "The problem of discontinuity as a result if using the discontinuous step function is mitigated by using a smooth and differential squashing function. This reinstitate the output $y$ using the activation function known as the *sigmoid*, which is differentiable. . With this development, the derivative of the sigmoid is introduced in the main relation: $\\delta w_i = \\alpha\\sigma'(a)(t^p-y^p)x_i^p$ where $\\sigma'(a)$ or $\\frac{d\\sigma(a)}{da}$ is the derivative of the sigmoid with respect to the activation $a$ .... change to the weight (and the error) are communicated via the activation ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPLEMENTATION BASED ON DELTA RULE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MULTILAYER PARCEPTRON"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
